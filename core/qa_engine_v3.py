# -*- coding: utf-8 -*-
# Copyright 2026 Sakura-é¢‘é“æ€»ç»“åŠ©æ‰‹
#
# æœ¬é¡¹ç›®é‡‡ç”¨ GNU Affero General Public License Version 3.0 (AGPL-3.0) è®¸å¯ï¼Œ
# å¹¶é™„åŠ éå•†ä¸šä½¿ç”¨é™åˆ¶æ¡æ¬¾ã€‚
#
# - ç½²åï¼šå¿…é¡»æä¾›æœ¬é¡¹ç›®çš„åŸå§‹æ¥æºé“¾æ¥
# - éå•†ä¸šï¼šç¦æ­¢ä»»ä½•å•†ä¸šç”¨é€”å’Œåˆ†å‘
# - ç›¸åŒæ–¹å¼å…±äº«ï¼šè¡ç”Ÿä½œå“å¿…é¡»é‡‡ç”¨ç›¸åŒçš„è®¸å¯è¯
#
# æœ¬é¡¹ç›®æºä»£ç ï¼šhttps://github.com/Sakura520222/Sakura-Channel-Summary-Assistant
# è®¸å¯è¯å…¨æ–‡ï¼šå‚è§ LICENSE æ–‡ä»¶

"""
é—®ç­”å¼•æ“ v3.0.0 - é›†æˆå‘é‡æœç´¢å’Œé‡æ’åº
å®ç°è¯­ä¹‰æ£€ç´¢ + RAGæ¶æ„
"""

import logging
from typing import Dict, Any, List, Optional
from .database import get_db_manager
from .intent_parser import get_intent_parser
from .memory_manager import get_memory_manager
from .vector_store import get_vector_store
from .reranker import get_reranker
from .ai_client import client_llm
from .settings import get_llm_model

logger = logging.getLogger(__name__)


class QAEngineV3:
    """é—®ç­”å¼•æ“ v3.0.0 - å‘é‡æœç´¢ç‰ˆæœ¬"""

    def __init__(self):
        """åˆå§‹åŒ–é—®ç­”å¼•æ“"""
        self.db = get_db_manager()
        self.intent_parser = get_intent_parser()
        self.memory_manager = get_memory_manager()
        self.vector_store = get_vector_store()
        self.reranker = get_reranker()
        logger.info("é—®ç­”å¼•æ“v3.0.0åˆå§‹åŒ–å®Œæˆ")

    async def process_query(self, query: str, user_id: int) -> str:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            user_id: ç”¨æˆ·ID

        Returns:
            å›ç­”æ–‡æœ¬
        """
        try:
            logger.info(f"å¤„ç†æŸ¥è¯¢: user_id={user_id}, query={query}")

            # 1. è§£ææŸ¥è¯¢æ„å›¾
            parsed = self.intent_parser.parse_query(query)
            logger.info(f"æŸ¥è¯¢æ„å›¾: {parsed['intent']}, ç½®ä¿¡åº¦: {parsed['confidence']}")

            # 2. æ ¹æ®æ„å›¾å¤„ç†
            intent = parsed["intent"]

            if intent == "status":
                return await self._handle_status_query()
            elif intent == "stats":
                return await self._handle_stats_query(parsed)
            else:
                return await self._handle_content_query_v3(parsed)

        except Exception as e:
            logger.error(f"å¤„ç†æŸ¥è¯¢å¤±è´¥: {type(e).__name__}: {e}", exc_info=True)
            return "âŒ å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™ï¼Œè¯·ç¨åé‡è¯•ã€‚"

    async def _handle_status_query(self) -> str:
        """å¤„ç†çŠ¶æ€æŸ¥è¯¢"""
        from .quota_manager import get_quota_manager
        quota_mgr = get_quota_manager()
        status = quota_mgr.get_system_status()

        vector_stats = self.vector_store.get_stats() if self.vector_store.is_available() else {}

        vector_info = ""
        if vector_stats.get("available"):
            total_vectors = vector_stats.get("total_vectors", 0)
            vector_info = f"\nâ€¢ å‘é‡æ€»ç»“æ•°: {total_vectors} æ¡"

        return f"""ğŸ“Š ç³»ç»ŸçŠ¶æ€

â€¢ æ¯æ—¥æ€»é™é¢: {status['daily_limit']} æ¬¡
â€¢ ä»Šæ—¥å·²ä½¿ç”¨: {status['used_today']} æ¬¡
â€¢ ä»Šæ—¥å‰©ä½™: {status['remaining']} æ¬¡
â€¢ ä½¿ç”¨ç‡: {status['utilization']}{vector_info}

ğŸ’¡ æ¯æ—¥00:00è‡ªåŠ¨é‡ç½®"""

    async def _handle_stats_query(self, parsed: Dict[str, Any]) -> str:
        """å¤„ç†ç»Ÿè®¡æŸ¥è¯¢"""
        stats = self.db.get_statistics()

        return f"""ğŸ“ˆ æ•°æ®ç»Ÿè®¡

â€¢ æ€»æ€»ç»“æ•°: {stats['total_count']} æ¡
â€¢ æ€»æ¶ˆæ¯æ•°: {stats['total_messages']:,} æ¡
â€¢ å¹³å‡æ¶ˆæ¯æ•°: {stats['avg_messages']} æ¡/æ€»ç»“
â€¢ æœ¬å‘¨æ€»ç»“: {stats['week_count']} æ¡
â€¢ æœ¬æœˆæ€»ç»“: {stats['month_count']} æ¡

ğŸ“Š ç±»å‹åˆ†å¸ƒ:""" + "\n".join(
            f"  â€¢ {t}: {c} æ¡" for t, c in stats.get('type_stats', {}).items()
        )

    async def _handle_content_query_v3(self, parsed: Dict[str, Any]) -> str:
        """
        å¤„ç†å†…å®¹æŸ¥è¯¢ï¼ˆv3.0.0å‘é‡æœç´¢ç‰ˆæœ¬ï¼‰

        å®ç°æ··åˆæ£€ç´¢ç­–ç•¥ï¼š
        1. è¯­ä¹‰æ£€ç´¢ï¼ˆDenseï¼‰
        2. å…³é”®è¯æ£€ç´¢ï¼ˆSparseï¼‰ä½œä¸ºå¤‡é€‰
        3. RRFèåˆ
        4. é‡æ’åº
        """
        try:
            query = parsed["original_query"]
            keywords = parsed.get("keywords", [])
            time_range = parsed.get("time_range", 7)

            # æ­¥éª¤1: è¯­ä¹‰æ£€ç´¢ï¼ˆå¬å›Top-20ï¼‰
            semantic_results = []
            if self.vector_store.is_available():
                try:
                    semantic_results = self.vector_store.search_similar(
                        query=query,
                        top_k=20
                    )
                    logger.info(f"è¯­ä¹‰æ£€ç´¢: æ‰¾åˆ° {len(semantic_results)} æ¡ç»“æœ")
                except Exception as e:
                    logger.error(f"è¯­ä¹‰æ£€ç´¢å¤±è´¥: {e}")

            # æ­¥éª¤2: å…³é”®è¯æ£€ç´¢ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
            keyword_results = []
            if keywords or not semantic_results:
                try:
                    from datetime import datetime, timezone, timedelta
                    # ç¡®ä¿time_rangeä¸ä¸ºNone
                    search_days = time_range if time_range is not None else 7
                    end_date = datetime.now(timezone.utc)
                    start_date = end_date - timedelta(days=search_days)

                    keyword_results = self.memory_manager.search_summaries(
                        keywords=keywords,
                        time_range_days=search_days,
                        limit=10
                    )
                    logger.info(f"å…³é”®è¯æ£€ç´¢: æ‰¾åˆ° {len(keyword_results)} æ¡ç»“æœ")
                except Exception as e:
                    logger.error(f"å…³é”®è¯æ£€ç´¢å¤±è´¥: {e}")

            # æ­¥éª¤3: èåˆç»“æœ
            if semantic_results and keyword_results:
                # ä½¿ç”¨RRFèåˆ
                final_candidates = self._rrf_fusion(semantic_results, keyword_results)
                logger.info(f"RRFèåˆ: {len(final_candidates)} æ¡ç»“æœ")
            elif semantic_results:
                # åªä½¿ç”¨è¯­ä¹‰æ£€ç´¢ç»“æœ
                final_candidates = semantic_results
            elif keyword_results:
                # åªä½¿ç”¨å…³é”®è¯æ£€ç´¢ç»“æœ
                final_candidates = [
                    {
                        'summary_id': r['id'],
                        'summary_text': r['summary_text'],
                        'metadata': {
                            'channel_id': r.get('channel_id'),
                            'channel_name': r.get('channel_name'),
                            'created_at': r.get('created_at')
                        }
                    }
                    for r in keyword_results
                ]
            else:
                # éƒ½æ²¡æœ‰ç»“æœ
                return f"ğŸ” æœªæ‰¾åˆ°ç›¸å…³æ€»ç»“ã€‚\n\nğŸ’¡ æç¤ºï¼šå°è¯•è°ƒæ•´å…³é”®è¯æˆ–æ—¶é—´èŒƒå›´ã€‚"

            # æ­¥éª¤4: é‡æ’åºï¼ˆTop-20 â†’ Top-5ï¼‰
            if self.reranker.is_available() and len(final_candidates) > 5:
                try:
                    final_candidates = self.reranker.rerank(query, final_candidates, top_k=5)
                    logger.info(f"é‡æ’åºå®Œæˆ: ä¿ç•™ {len(final_candidates)} æ¡ç»“æœ")
                except Exception as e:
                    logger.error(f"é‡æ’åºå¤±è´¥: {e}")
                    final_candidates = final_candidates[:5]
            else:
                final_candidates = final_candidates[:5]

            # æ­¥éª¤5: AIç”Ÿæˆå›ç­”ï¼ˆRAGï¼‰
            answer = await self._generate_answer_with_rag(
                query=query,
                summaries=final_candidates,
                keywords=keywords
            )

            return answer

        except Exception as e:
            logger.error(f"å¤„ç†å†…å®¹æŸ¥è¯¢å¤±è´¥: {type(e).__name__}: {e}", exc_info=True)
            return "âŒ æŸ¥è¯¢å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚"

    def _rrf_fusion(self, semantic_results: List[Dict], 
                   keyword_results: List[Dict], k: int = 60) -> List[Dict]:
        """
        Reciprocal Rank Fusion (RRF) èåˆç®—æ³•

        Args:
            semantic_results: è¯­ä¹‰æ£€ç´¢ç»“æœ
            keyword_results: å…³é”®è¯æ£€ç´¢ç»“æœ
            k: RRFå¸¸æ•°ï¼Œé»˜è®¤60

        Returns:
            èåˆåçš„ç»“æœåˆ—è¡¨
        """
        # æ„å»ºIDåˆ°ç»“æœçš„æ˜ å°„
        result_map = {}

        # å¤„ç†è¯­ä¹‰æ£€ç´¢ç»“æœ
        for rank, result in enumerate(semantic_results, 1):
            summary_id = result['summary_id']
            score = 1.0 / (k + rank)
            result_map[summary_id] = {
                'summary': result,
                'score': score,
                'source': 'semantic'
            }

        # å¤„ç†å…³é”®è¯æ£€ç´¢ç»“æœ
        for rank, result in enumerate(keyword_results, 1):
            summary_id = result['id']
            score = 1.0 / (k + rank)
            
            if summary_id in result_map:
                # åˆå¹¶åˆ†æ•°
                result_map[summary_id]['score'] += score
                result_map[summary_id]['source'] = 'hybrid'
            else:
                # æ·»åŠ æ–°ç»“æœ
                result_map[summary_id] = {
                    'summary': {
                        'summary_id': result['id'],
                        'summary_text': result['summary_text'],
                        'metadata': {
                            'channel_id': result.get('channel_id'),
                            'channel_name': result.get('channel_name'),
                            'created_at': result.get('created_at')
                        }
                    },
                    'score': score,
                    'source': 'keyword'
                }

        # æŒ‰åˆ†æ•°æ’åº
        sorted_results = sorted(
            result_map.values(),
            key=lambda x: x['score'],
            reverse=True
        )

        # è¿”å›èåˆåçš„ç»“æœåˆ—è¡¨
        return [item['summary'] for item in sorted_results]

    async def _generate_answer_with_rag(self, query: str,
                                        summaries: List[Dict[str, Any]],
                                        keywords: List[str] = None) -> str:
        """
        ä½¿ç”¨RAGç”Ÿæˆå›ç­”

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            summaries: ç›¸å…³æ€»ç»“åˆ—è¡¨
            keywords: å…³é”®è¯

        Returns:
            ç”Ÿæˆçš„å›ç­”
        """
        try:
            # å‡†å¤‡ä¸Šä¸‹æ–‡
            context = self._prepare_rag_context(summaries)

            # è·å–é¢‘é“ç”»åƒ
            channel_ids = list(set(
                s.get('metadata', {}).get('channel_id') or s.get('channel_id', '')
                for s in summaries
            ))
            channel_context = ""
            if len(channel_ids) == 1 and channel_ids[0]:
                channel_context = self.memory_manager.get_channel_context(channel_ids[0])
            elif len(channel_ids) > 1:
                channel_context = "å¤šé¢‘é“ç»¼åˆæŸ¥è¯¢"

            # æ„å»ºæç¤ºè¯
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èµ„è®¯åŠ©æ‰‹ï¼Œè´Ÿè´£æ ¹æ®å†å²æ€»ç»“å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

{channel_context}

ç”¨æˆ·æŸ¥è¯¢ï¼š{query}

ç›¸å…³å†å²æ€»ç»“ï¼ˆå…±{len(summaries)}æ¡ï¼Œå·²é€šè¿‡è¯­ä¹‰æœç´¢å’Œé‡æ’åºç²¾é€‰ï¼‰ï¼š
{context}

è¦æ±‚ï¼ˆä¸¥æ ¼éµå¾ªï¼‰ï¼š
1. åŸºäºä¸Šè¿°æ€»ç»“å†…å®¹å›ç­”é—®é¢˜ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. å¦‚æœæ€»ç»“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜
3. ä½¿ç”¨æ¸…æ™°çš„ç»“æ„å’Œè¦ç‚¹
4. è¯­è¨€ç®€æ´ä¸“ä¸š
5. **Markdownæ ¼å¼è¦æ±‚**ï¼š
   - ç²—ä½“ï¼šä½¿ç”¨ **æ–‡æœ¬** ï¼ˆæ³¨æ„ä¸¤è¾¹å„ä¸¤ä¸ªæ˜Ÿå·ï¼‰
   - æ–œä½“ï¼šä½¿ç”¨ *æ–‡æœ¬* ï¼ˆæ³¨æ„ä¸¤è¾¹å„ä¸€ä¸ªæ˜Ÿå·ï¼‰
   - ä»£ç ï¼šä½¿ç”¨ `ä»£ç ` ï¼ˆåå¼•å·ï¼‰
   - **ç¦æ­¢ä½¿ç”¨ # æ ‡é¢˜æ ¼å¼**
   - åˆ—è¡¨ï¼šä½¿ç”¨ - æˆ– â€¢ å¼€å¤´
   - é“¾æ¥ï¼šä½¿ç”¨ [æ–‡æœ¬](URL) æ ¼å¼
   - **ç¦æ­¢ä½¿ç”¨æœªé…å¯¹çš„æ˜Ÿå·ã€ä¸‹åˆ’çº¿æˆ–åå¼•å·**
   - **æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦å¿…é¡»æˆå¯¹å‡ºç°**

è¯·ç”¨ä¸¥æ ¼çš„Markdownæ ¼å¼å›ç­”ï¼ˆä¸ä½¿ç”¨#æ ‡é¢˜ï¼‰ï¼š"""

            logger.info(f"è°ƒç”¨AIç”Ÿæˆå›ç­”ï¼ˆRAGï¼‰ï¼Œæ€»ç»“æ•°: {len(summaries)}")

            response = client_llm.chat.completions.create(
                model=get_llm_model(),
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èµ„è®¯åŠ©æ‰‹ï¼Œæ“…é•¿ä»å†å²è®°å½•ä¸­æå–å…³é”®ä¿¡æ¯å¹¶å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7
            )

            answer = response.choices[0].message.content.strip()
            logger.info(f"AIå›ç­”ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(answer)}å­—ç¬¦")

            # æ·»åŠ æ¥æºä¿¡æ¯
            source_info = self._format_source_info_v3(summaries)
            return f"{answer}\n\n{source_info}"

        except Exception as e:
            error_type = type(e).__name__
            error_msg = str(e)
            
            logger.error(f"AIç”Ÿæˆå›ç­”å¤±è´¥: {error_type}: {error_msg}", exc_info=True)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å†…å®¹å®¡æ ¸æ‹¦æˆª
            if 'Moderation Block' in error_msg or 'content_filter' in error_msg:
                return """âŒ æŠ±æ­‰ï¼Œæ‚¨çš„æŸ¥è¯¢åŒ…å«ä¸å½“å†…å®¹ï¼Œå·²è¢«ç³»ç»Ÿæ‹¦æˆªã€‚

ğŸ’¡ æˆ‘æ˜¯ä¸€ä¸ªé¢‘é“æ€»ç»“åŠ©æ‰‹ï¼Œåªèƒ½å›ç­”ä¸é¢‘é“å†å²æ€»ç»“ç›¸å…³çš„é—®é¢˜ã€‚

ğŸ“š è¯·å°è¯•ï¼š
â€¢ è¯¢é—®é¢‘é“æœ€è¿‘å‘ç”Ÿäº†ä»€ä¹ˆ
â€¢ æŸ¥è¯¢ç‰¹å®šä¸»é¢˜çš„å†å²è®°å½•
â€¢ äº†è§£é¢‘é“ç»Ÿè®¡æ•°æ®
â€¢ æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€"""
            
            # å…¶ä»–é”™è¯¯ï¼šé™çº§æ–¹æ¡ˆï¼Œç›´æ¥è¿”å›æ€»ç»“æ‘˜è¦
            return self._fallback_answer_v3(summaries)

    def _prepare_rag_context(self, summaries: List[Dict[str, Any]]) -> str:
        """å‡†å¤‡RAGä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context_parts = []

        for i, summary in enumerate(summaries[:5], 1):
            # ä»metadataæˆ–ç›´æ¥å­—æ®µè·å–ä¿¡æ¯
            metadata = summary.get('metadata', {})
            channel_name = metadata.get('channel_name') or summary.get('channel_name', 'æœªçŸ¥é¢‘é“')
            created_at = metadata.get('created_at') or summary.get('created_at', '')
            summary_text = summary.get('summary_text', '')

            # æå–æ‘˜è¦ï¼ˆå‰500å­—ç¬¦ï¼‰
            text_preview = summary_text[:500] + "..." if len(summary_text) > 500 else summary_text

            # æ·»åŠ ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
            score_info = ""
            if 'similarity' in summary:
                score_info = f" [ç›¸ä¼¼åº¦: {summary['similarity']:.2f}]"
            if 'rerank_score' in summary:
                score_info += f" [é‡æ’åˆ†: {summary['rerank_score']:.2f}]"

            context_parts.append(
                f"[{i}] {channel_name} ({created_at}){score_info}\n{text_preview}"
            )

        return "\n\n".join(context_parts)

    def _format_source_info_v3(self, summaries: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–æ¥æºä¿¡æ¯ï¼ˆv3ç‰ˆæœ¬ï¼‰"""
        channels = {}
        for s in summaries:
            metadata = s.get('metadata', {})
            channel_id = metadata.get('channel_id') or s.get('channel_id', '')
            channel_name = metadata.get('channel_name') or s.get('channel_name', 'æœªçŸ¥é¢‘é“')
            
            if channel_id not in channels:
                channels[channel_id] = {
                    'name': channel_name,
                    'count': 0
                }
            channels[channel_id]['count'] += 1

        sources = [f"â€¢ {info['name']}: {info['count']}æ¡"
                  for info in channels.values()]

        return f"ğŸ“š æ•°æ®æ¥æº: {len(sources)}ä¸ªé¢‘é“\n" + "\n".join(sources)

    def _fallback_answer_v3(self, summaries: List[Dict[str, Any]]) -> str:
        """é™çº§æ–¹æ¡ˆï¼šç›´æ¥è¿”å›æ€»ç»“æ‘˜è¦ï¼ˆv3ç‰ˆæœ¬ï¼‰"""
        result = "ğŸ“‹ ç›¸å…³æ€»ç»“æ‘˜è¦ï¼š\n\n"

        for i, summary in enumerate(summaries[:3], 1):
            metadata = summary.get('metadata', {})
            channel_name = metadata.get('channel_name') or summary.get('channel_name', 'æœªçŸ¥é¢‘é“')
            created_at = (metadata.get('created_at') or summary.get('created_at', ''))[:10]
            text = summary.get('summary_text', '')[:200]

            result += f"{i}. **{channel_name}** ({created_at})\n{text}...\n\n"

        return result


# åˆ›å»ºå…¨å±€é—®ç­”å¼•æ“v3å®ä¾‹
qa_engine_v3 = None

def get_qa_engine_v3():
    """è·å–å…¨å±€é—®ç­”å¼•æ“v3å®ä¾‹"""
    global qa_engine_v3
    if qa_engine_v3 is None:
        qa_engine_v3 = QAEngineV3()
    return qa_engine_v3