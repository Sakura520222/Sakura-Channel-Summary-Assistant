# -*- coding: utf-8 -*-
# Copyright 2026 Sakura-Bot
#
# æœ¬é¡¹ç›®é‡‡ç”¨ GNU Affero General Public License Version 3.0 (AGPL-3.0) è®¸å¯ï¼Œ
# å¹¶é™„åŠ éå•†ä¸šä½¿ç”¨é™åˆ¶æ¡æ¬¾ã€‚
#
# - ç½²åï¼šå¿…é¡»æä¾›æœ¬é¡¹ç›®çš„åŸå§‹æ¥æºé“¾æ¥
# - éå•†ä¸šï¼šç¦æ­¢ä»»ä½•å•†ä¸šç”¨é€”å’Œåˆ†å‘
# - ç›¸åŒæ–¹å¼å…±äº«ï¼šè¡ç”Ÿä½œå“å¿…é¡»é‡‡ç”¨ç›¸åŒçš„è®¸å¯è¯
#
# æœ¬é¡¹ç›®æºä»£ç ï¼šhttps://github.com/Sakura520222/Sakura-Bot
# è®¸å¯è¯å…¨æ–‡ï¼šå‚è§ LICENSE æ–‡ä»¶

"""
é—®ç­”å¼•æ“ v3.1.0 - é›†æˆå‘é‡æœç´¢å’Œé‡æ’åº
å®ç°è¯­ä¹‰æ£€ç´¢ + RAGæ¶æ„ + å¤šè½®å¯¹è¯æŸ¥è¯¢æ”¹å†™
"""

import logging
import re
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List, Optional

from .ai_client import client_llm
from .config import get_qa_bot_persona
from .conversation_manager import get_conversation_manager
from .database import get_db_manager
from .intent_parser import get_intent_parser
from .memory_manager import get_memory_manager
from .reranker import get_reranker
from .settings import get_llm_model
from .vector_store import get_vector_store

logger = logging.getLogger(__name__)

# åˆ¤æ–­æŸ¥è¯¢æ˜¯å¦å«æœ‰ä»£è¯/æŒ‡ä»£è¯ï¼Œéœ€è¦è¿›è¡ŒæŸ¥è¯¢æ”¹å†™
PRONOUN_PATTERNS = re.compile(
    r'å®ƒ|ä»–|å¥¹|è¿™ä¸ª|é‚£ä¸ª|è¿™äº›|é‚£äº›|æ­¤|å½¼|å‰è€…|åè€…|ä¸Šé¢|ä¸Šè¿°|åˆšæ‰|ä¹‹å‰|ç»§ç»­|è¿˜æœ‰|é‚£ä¹ˆ|é‚£|è¿™'
)

# ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿ï¼ˆä½¿ç”¨å ä½ç¬¦ï¼Œäººæ ¼æè¿°ä¼šåŠ¨æ€æ³¨å…¥ï¼‰
BASE_SYSTEM_TEMPLATE = """{persona_description}

---

## æ ¸å¿ƒä»»åŠ¡
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼Œè´Ÿè´£æ ¹æ®é¢‘é“å†å²æ€»ç»“å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

## æ ¸å¿ƒçº¦æŸï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
1. **åŸºäºäº‹å®**ï¼šä¸¥æ ¼åŸºäºæä¾›çš„å†å²æ€»ç»“å†…å®¹å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. **ä¸Šä¸‹æ–‡ç†è§£**ï¼šå¦‚æœæœ‰å¯¹è¯å†å²ï¼Œä¼˜å…ˆåˆ©ç”¨ä¸Šä¸‹æ–‡ç†è§£ä»£è¯æŒ‡ä»£ï¼ˆå¦‚"å®ƒ"ã€"é‚£ä¸ª"ã€"è¿™ä¸ª"ç­‰ï¼‰
3. **æ˜ç¡®å›ç­”**ï¼šæ€»ç»“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯æ—¶ï¼Œæ˜ç¡®è¯´æ˜
4. **ç»“æ„æ¸…æ™°**ï¼šä½¿ç”¨æ¸…æ™°çš„ç»“æ„å’Œè¦ç‚¹ï¼Œè¯­è¨€ç®€æ´ä¸“ä¸š
5. **Markdownè§„èŒƒ**ï¼š
   - ç²—ä½“ï¼šä½¿ç”¨ **æ–‡æœ¬** ï¼ˆä¸¤è¾¹å„ä¸¤ä¸ªæ˜Ÿå·ï¼‰
   - æ–œä½“ï¼šä½¿ç”¨ *æ–‡æœ¬* ï¼ˆä¸¤è¾¹å„ä¸€ä¸ªæ˜Ÿå·ï¼‰
   - ä»£ç ï¼šä½¿ç”¨ `ä»£ç ` ï¼ˆåå¼•å·ï¼‰
   - **ç¦æ­¢ä½¿ç”¨ # æ ‡é¢˜æ ¼å¼**
   - åˆ—è¡¨ï¼šä½¿ç”¨ - æˆ– â€¢ å¼€å¤´
   - é“¾æ¥ï¼šä½¿ç”¨ [æ–‡æœ¬](URL) æ ¼å¼
   - **ç¦æ­¢ä½¿ç”¨æœªé…å¯¹çš„æ˜Ÿå·ã€ä¸‹åˆ’çº¿æˆ–åå¼•å·**

## å½“å‰ä¸Šä¸‹æ–‡
{channel_context}{conversation_context}
"""

# æŸ¥è¯¢æ”¹å†™ä¸“ç”¨ç³»ç»Ÿæç¤º
REWRITE_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢æ”¹å†™åŠ©æ‰‹ã€‚
ç»™å®šå¤šè½®å¯¹è¯å†å²å’Œç”¨æˆ·çš„æœ€æ–°é—®é¢˜ï¼Œå°†ç”¨æˆ·çš„æœ€æ–°é—®é¢˜æ”¹å†™ä¸ºä¸€ä¸ª**ç‹¬ç«‹ã€å®Œæ•´ã€ä¸ä¾èµ–ä¸Šä¸‹æ–‡çš„æœç´¢æŸ¥è¯¢**ã€‚
è¦æ±‚ï¼š
- è§£ææ‰€æœ‰ä»£è¯ï¼ˆ"å®ƒ"ã€"é‚£ä¸ª"ã€"è¿™ä¸ª"ç­‰ï¼‰ï¼Œæ›¿æ¢ä¸ºæ˜ç¡®çš„å®ä½“åç§°
- ä¿ç•™åŸå§‹æ„å›¾å’Œå…³é”®ä¿¡æ¯
- ä»…è¾“å‡ºæ”¹å†™åçš„æŸ¥è¯¢æ–‡æœ¬ï¼Œä¸è¦ä»»ä½•è§£é‡Šæˆ–å‰ç¼€
- å¦‚æœæœ€æ–°é—®é¢˜å·²ç»è¶³å¤Ÿç‹¬ç«‹æ¸…æ™°ï¼Œç›´æ¥åŸæ ·è¿”å›"""


class QAEngineV3:
    """é—®ç­”å¼•æ“ v3.1.0 - å‘é‡æœç´¢ + å¤šè½®å¯¹è¯æ”¯æŒ + æŸ¥è¯¢æ”¹å†™"""

    def __init__(self):
        """åˆå§‹åŒ–é—®ç­”å¼•æ“"""
        self.db = get_db_manager()
        self.intent_parser = get_intent_parser()
        self.memory_manager = get_memory_manager()
        self.vector_store = get_vector_store()
        self.reranker = get_reranker()
        self.conversation_mgr = get_conversation_manager()
        logger.info("é—®ç­”å¼•æ“v3.1.0åˆå§‹åŒ–å®Œæˆï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ + æŸ¥è¯¢æ”¹å†™ï¼‰")

    async def process_query(self, query: str, user_id: int) -> str:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            user_id: ç”¨æˆ·ID

        Returns:
            å›ç­”æ–‡æœ¬
        """
        try:
            logger.info(f"å¤„ç†æŸ¥è¯¢: user_id={user_id}, query={query}")

            # 1. è·å–æˆ–åˆ›å»ºä¼šè¯
            session_id, is_new_session = self.conversation_mgr.get_or_create_session(user_id)

            # 2. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
            self.conversation_mgr.save_message(
                user_id=user_id,
                session_id=session_id,
                role='user',
                content=query
            )

            # 3. è§£ææŸ¥è¯¢æ„å›¾
            parsed = self.intent_parser.parse_query(query)
            logger.info(f"æŸ¥è¯¢æ„å›¾: {parsed['intent']}, ç½®ä¿¡åº¦: {parsed['confidence']}")

            # 4. æ ¹æ®æ„å›¾å¤„ç†
            intent = parsed["intent"]

            if intent == "status":
                answer = await self._handle_status_query()
            elif intent == "stats":
                answer = await self._handle_stats_query(parsed)
            else:
                answer = await self._handle_content_query_v3(parsed, user_id, session_id, is_new_session)

            # 5. ä¿å­˜åŠ©æ‰‹å›å¤
            self.conversation_mgr.save_message(
                user_id=user_id,
                session_id=session_id,
                role='assistant',
                content=answer
            )

            return answer

        except Exception as e:
            logger.error(f"å¤„ç†æŸ¥è¯¢å¤±è´¥: {type(e).__name__}: {e}", exc_info=True)
            return "âŒ å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™ï¼Œè¯·ç¨åé‡è¯•ã€‚"

    async def _handle_status_query(self) -> str:
        """å¤„ç†çŠ¶æ€æŸ¥è¯¢"""
        from .quota_manager import get_quota_manager
        quota_mgr = get_quota_manager()
        status = quota_mgr.get_system_status()

        vector_stats = self.vector_store.get_stats() if self.vector_store.is_available() else {}

        vector_info = ""
        if vector_stats.get("available"):
            total_vectors = vector_stats.get("total_vectors", 0)
            vector_info = f"\nâ€¢ å‘é‡æ€»ç»“æ•°: {total_vectors} æ¡"

        return f"""ğŸ“Š ç³»ç»ŸçŠ¶æ€

â€¢ æ¯æ—¥æ€»é™é¢: {status['daily_limit']} æ¬¡
â€¢ ä»Šæ—¥å‰©ä½™: {status['remaining']} æ¬¡{vector_info}

ğŸ’¡ æ¯æ—¥00:00è‡ªåŠ¨é‡ç½®"""

    async def _handle_stats_query(self, parsed: Dict[str, Any]) -> str:
        """å¤„ç†ç»Ÿè®¡æŸ¥è¯¢"""
        stats = self.db.get_statistics()

        return f"""ğŸ“ˆ æ•°æ®ç»Ÿè®¡

â€¢ æ€»æ€»ç»“æ•°: {stats['total_count']} æ¡
â€¢ æ€»æ¶ˆæ¯æ•°: {stats['total_messages']:,} æ¡
â€¢ å¹³å‡æ¶ˆæ¯æ•°: {stats['avg_messages']} æ¡/æ€»ç»“
â€¢ æœ¬å‘¨æ€»ç»“: {stats['week_count']} æ¡
â€¢ æœ¬æœˆæ€»ç»“: {stats['month_count']} æ¡

ğŸ“Š ç±»å‹åˆ†å¸ƒ:""" + "\n".join(
            f"  â€¢ {t}: {c} æ¡" for t, c in stats.get('type_stats', {}).items()
        )

    async def _handle_content_query_v3(self, parsed: Dict[str, Any],
                                       user_id: int, session_id: str,
                                       is_new_session: bool = False) -> str:
        """
        å¤„ç†å†…å®¹æŸ¥è¯¢ï¼ˆv3.1.0ï¼‰

        å®ç°æ··åˆæ£€ç´¢ç­–ç•¥ï¼š
        1. æŸ¥è¯¢æ”¹å†™ï¼ˆå¤šè½®å¯¹è¯ä¸­å«ä»£è¯æ—¶ï¼‰
        2. è¯­ä¹‰æ£€ç´¢ï¼ˆDenseï¼Œå«æ—¶é—´è¿‡æ»¤ï¼‰
        3. å…³é”®è¯æ£€ç´¢ï¼ˆSparseï¼‰ä½œä¸ºè¡¥å……
        4. RRFèåˆ
        5. Rerankerç²¾æ’
        6. RAGç”Ÿæˆï¼ˆå«å¯¹è¯å†å²ï¼‰
        """
        try:
            query = parsed["original_query"]
            keywords = parsed.get("keywords", [])
            time_range = parsed.get("time_range")  # å¯èƒ½ä¸º None

            # è·å–å¯¹è¯å†å²
            conversation_history = self.conversation_mgr.get_conversation_history(user_id, session_id)
            logger.debug(f"ç”¨æˆ· {user_id} çš„å¯¹è¯å†å²: {len(conversation_history)} æ¡")

            # â”€â”€ æ­¥éª¤0: æŸ¥è¯¢æ”¹å†™ï¼ˆå¤šè½®å¯¹è¯ + å«ä»£è¯æ—¶ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            search_query = query  # ç”¨äºæ£€ç´¢çš„æŸ¥è¯¢ï¼ˆå¯èƒ½è¢«æ”¹å†™ï¼‰
            query_rewritten = False

            if (not is_new_session
                    and len(conversation_history) >= 3  # è‡³å°‘æœ‰1è½®å†å²
                    and PRONOUN_PATTERNS.search(query)):
                try:
                    search_query = await self._rewrite_query(query, conversation_history)
                    if search_query != query:
                        query_rewritten = True
                        logger.info(f"æŸ¥è¯¢æ”¹å†™: '{query}' â†’ '{search_query}'")
                except Exception as e:
                    logger.warning(f"æŸ¥è¯¢æ”¹å†™å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢: {e}")
                    search_query = query

            # â”€â”€ æ­¥éª¤1: è®¡ç®—æ—¶é—´è¿‡æ»¤èŒƒå›´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            date_after: Optional[str] = None
            if time_range is not None:
                cutoff = datetime.now(timezone.utc) - timedelta(days=time_range)
                date_after = cutoff.isoformat()
                logger.info(f"æ—¶é—´è¿‡æ»¤: date_after={date_after[:10]}")

            # â”€â”€ æ­¥éª¤2: è¯­ä¹‰æ£€ç´¢ï¼ˆå¬å›Top-20ï¼Œå«æ—¶é—´è¿‡æ»¤ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            semantic_results = []
            if self.vector_store.is_available():
                try:
                    semantic_results = self.vector_store.search_similar(
                        query=search_query,
                        top_k=20,
                        date_after=date_after
                    )
                    logger.info(f"è¯­ä¹‰æ£€ç´¢: æ‰¾åˆ° {len(semantic_results)} æ¡ç»“æœ")
                except Exception as e:
                    logger.error(f"è¯­ä¹‰æ£€ç´¢å¤±è´¥: {e}")

            # â”€â”€ æ­¥éª¤3: å…³é”®è¯æ£€ç´¢ï¼ˆè¡¥å……æ–¹æ¡ˆï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            keyword_results = []
            # å½“è¯­ä¹‰æ£€ç´¢ç»“æœä¸è¶³æˆ–æœ‰æ˜ç¡®å…³é”®è¯æ—¶ï¼Œå¯ç”¨å…³é”®è¯æ£€ç´¢
            if keywords or len(semantic_results) < 5:
                try:
                    search_days = time_range if time_range is not None else 90
                    keyword_results = self.memory_manager.search_summaries(
                        keywords=keywords,
                        time_range_days=search_days,
                        limit=10
                    )
                    logger.info(f"å…³é”®è¯æ£€ç´¢: æ‰¾åˆ° {len(keyword_results)} æ¡ç»“æœ")
                except Exception as e:
                    logger.error(f"å…³é”®è¯æ£€ç´¢å¤±è´¥: {e}")

            # â”€â”€ æ­¥éª¤4: èåˆç»“æœ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if semantic_results and keyword_results:
                final_candidates = self._rrf_fusion(semantic_results, keyword_results)
                logger.info(f"RRFèåˆ: {len(final_candidates)} æ¡ç»“æœ")
            elif semantic_results:
                final_candidates = semantic_results
            elif keyword_results:
                final_candidates = [
                    {
                        'summary_id': r['id'],
                        'summary_text': r['summary_text'],
                        'metadata': {
                            'channel_id': r.get('channel_id'),
                            'channel_name': r.get('channel_name'),
                            'created_at': r.get('created_at')
                        }
                    }
                    for r in keyword_results
                ]
            else:
                if time_range is not None and time_range <= 7:
                    return (f"ğŸ” åœ¨æœ€è¿‘ {time_range} å¤©å†…æœªæ‰¾åˆ°ç›¸å…³æ€»ç»“ã€‚\n\n"
                            f"ğŸ’¡ æç¤ºï¼šå¯ä»¥å°è¯•æ‰©å¤§æ—¶é—´èŒƒå›´ï¼Œä¾‹å¦‚'æœ€è¿‘30å¤©å…³äº...'ã€‚")
                return "ğŸ” æœªæ‰¾åˆ°ç›¸å…³æ€»ç»“ã€‚\n\nğŸ’¡ æç¤ºï¼šå°è¯•è°ƒæ•´å…³é”®è¯æˆ–æ—¶é—´èŒƒå›´ã€‚"

            # â”€â”€ æ­¥éª¤5: é‡æ’åºï¼ˆTop-20 â†’ Top-5ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if self.reranker.is_available() and len(final_candidates) > 5:
                try:
                    final_candidates = self.reranker.rerank(search_query, final_candidates, top_k=5)
                    logger.info(f"é‡æ’åºå®Œæˆ: ä¿ç•™ {len(final_candidates)} æ¡ç»“æœ")
                except Exception as e:
                    logger.error(f"é‡æ’åºå¤±è´¥: {e}")
                    final_candidates = final_candidates[:5]
            else:
                final_candidates = final_candidates[:5]

            # â”€â”€ æ­¥éª¤6: AIç”Ÿæˆå›ç­”ï¼ˆRAG + å¯¹è¯å†å²ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            answer = await self._generate_answer_with_rag(
                query=query,                        # åŸå§‹æŸ¥è¯¢ï¼ˆç”¨äºAIç†è§£ç”¨æˆ·æ„å›¾ï¼‰
                search_query=search_query,          # æ”¹å†™åæŸ¥è¯¢ï¼ˆç”¨äºè¯´æ˜æ£€ç´¢ä¾æ®ï¼‰
                summaries=final_candidates,
                keywords=keywords,
                conversation_history=conversation_history,
                query_rewritten=query_rewritten
            )

            # æ–°ä¼šè¯æ—¶åŠ ä¸Šå¼•å¯¼è¯­
            if is_new_session:
                answer = "ğŸƒ *å¼€å§‹æ–°çš„å¯¹è¯ã€‚*\n\n" + answer

            return answer

        except Exception as e:
            logger.error(f"å¤„ç†å†…å®¹æŸ¥è¯¢å¤±è´¥: {type(e).__name__}: {e}", exc_info=True)
            return "âŒ æŸ¥è¯¢å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚"

    async def _rewrite_query(self, query: str,
                             conversation_history: List[Dict]) -> str:
        """
        åˆ©ç”¨LLMå°†å«ä»£è¯çš„æŸ¥è¯¢æ”¹å†™ä¸ºç‹¬ç«‹å®Œæ•´çš„æ£€ç´¢æŸ¥è¯¢

        Args:
            query: åŸå§‹ç”¨æˆ·æŸ¥è¯¢ï¼ˆå¯èƒ½å«ä»£è¯ï¼‰
            conversation_history: å½“å‰ä¼šè¯å†å²

        Returns:
            æ”¹å†™åçš„æŸ¥è¯¢å­—ç¬¦ä¸²
        """
        # åªå–æœ€è¿‘4æ¡å†å²ï¼ˆé¿å…tokenæµªè´¹ï¼‰
        recent_history = conversation_history[-4:] if len(conversation_history) > 4 else conversation_history
        # æ’é™¤æœ€åä¸€æ¡ï¼ˆå°±æ˜¯å½“å‰çš„ç”¨æˆ·æŸ¥è¯¢ï¼‰
        context_history = recent_history[:-1] if len(recent_history) > 1 else []

        if not context_history:
            return query

        # æ„å»ºå¯¹è¯å†å²æ–‡æœ¬
        history_text = self.conversation_mgr.format_conversation_context(context_history)

        response = client_llm.chat.completions.create(
            model=get_llm_model(),
            messages=[
                {"role": "system", "content": REWRITE_SYSTEM_PROMPT},
                {"role": "user", "content": f"å¯¹è¯å†å²ï¼š\n{history_text}\n\nç”¨æˆ·æœ€æ–°é—®é¢˜ï¼š{query}\n\næ”¹å†™åçš„ç‹¬ç«‹æŸ¥è¯¢ï¼š"}
            ],
            temperature=0.1,
            max_tokens=200
        )

        rewritten = response.choices[0].message.content.strip()
        # å®‰å…¨æ£€æŸ¥ï¼šè‹¥æ”¹å†™ç»“æœè¿‡é•¿æˆ–ä¸ºç©ºï¼Œå›é€€åˆ°åŸå§‹æŸ¥è¯¢
        if not rewritten or len(rewritten) > 200:
            return query
        return rewritten

    def _rrf_fusion(self, semantic_results: List[Dict],
                   keyword_results: List[Dict], k: int = 60) -> List[Dict]:
        """
        Reciprocal Rank Fusion (RRF) èåˆç®—æ³•

        Args:
            semantic_results: è¯­ä¹‰æ£€ç´¢ç»“æœ
            keyword_results: å…³é”®è¯æ£€ç´¢ç»“æœ
            k: RRFå¸¸æ•°ï¼Œé»˜è®¤60

        Returns:
            èåˆåçš„ç»“æœåˆ—è¡¨
        """
        result_map = {}

        # å¤„ç†è¯­ä¹‰æ£€ç´¢ç»“æœ
        for rank, result in enumerate(semantic_results, 1):
            summary_id = result['summary_id']
            score = 1.0 / (k + rank)
            result_map[summary_id] = {
                'summary': result,
                'score': score,
                'source': 'semantic'
            }

        # å¤„ç†å…³é”®è¯æ£€ç´¢ç»“æœ
        for rank, result in enumerate(keyword_results, 1):
            summary_id = result['id']
            score = 1.0 / (k + rank)

            if summary_id in result_map:
                result_map[summary_id]['score'] += score
                result_map[summary_id]['source'] = 'hybrid'
            else:
                result_map[summary_id] = {
                    'summary': {
                        'summary_id': result['id'],
                        'summary_text': result['summary_text'],
                        'metadata': {
                            'channel_id': result.get('channel_id'),
                            'channel_name': result.get('channel_name'),
                            'created_at': result.get('created_at')
                        }
                    },
                    'score': score,
                    'source': 'keyword'
                }

        sorted_results = sorted(
            result_map.values(),
            key=lambda x: x['score'],
            reverse=True
        )

        return [item['summary'] for item in sorted_results]

    def _build_rag_prompts(self, query: str, summaries: List[Dict[str, Any]],
                           keywords: List[str] = None,
                           conversation_history: List[Dict] = None,
                           search_query: str = None,
                           query_rewritten: bool = False) -> tuple:
        """
        æ„å»º RAG æ‰€éœ€çš„ system_prompt å’Œ user_promptï¼ˆä¾›æµå¼ä¸éæµå¼å…±ç”¨ï¼‰

        Returns:
            (system_prompt, user_prompt)
        """
        context = self._prepare_rag_context(summaries)

        channel_ids = list(set(
            s.get('metadata', {}).get('channel_id') or s.get('channel_id', '')
            for s in summaries
        ))
        channel_context = ""
        if len(channel_ids) == 1 and channel_ids[0]:
            channel_context = self.memory_manager.get_channel_context(channel_ids[0])
        elif len(channel_ids) > 1:
            channel_context = "å¤šé¢‘é“ç»¼åˆæŸ¥è¯¢"

        conversation_context = ""
        if conversation_history and len(conversation_history) > 0:
            history_excluding_current = (
                conversation_history[:-1] if len(conversation_history) > 1 else []
            )
            if history_excluding_current:
                conversation_context = self.conversation_mgr.format_conversation_context(
                    history_excluding_current
                )
                conversation_context = f"\nã€å¯¹è¯å†å²ã€‘\n{conversation_context}\n"

        persona_description = get_qa_bot_persona()
        system_prompt = BASE_SYSTEM_TEMPLATE.format(
            persona_description=persona_description,
            channel_context=channel_context,
            conversation_context=conversation_context
        )

        rewrite_note = ""
        if query_rewritten and search_query and search_query != query:
            rewrite_note = f"\nï¼ˆå·²æ ¹æ®å¯¹è¯ä¸Šä¸‹æ–‡å°†æŸ¥è¯¢ç†è§£ä¸ºï¼šã€Œ{search_query}ã€ï¼‰"

        user_prompt = (
            f"ç”¨æˆ·å½“å‰æŸ¥è¯¢ï¼š{query}{rewrite_note}\n\n"
            f"ç›¸å…³å†å²æ€»ç»“ï¼ˆå…±{len(summaries)}æ¡ï¼Œå·²é€šè¿‡è¯­ä¹‰æœç´¢å’Œé‡æ’åºç²¾é€‰ï¼‰ï¼š\n"
            f"{context}\n\n"
            f"è¯·æ ¹æ®ä¸Šè¿°æ€»ç»“å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
        )

        return system_prompt, user_prompt

    async def _generate_answer_with_rag(self, query: str,
                                        summaries: List[Dict[str, Any]],
                                        keywords: List[str] = None,
                                        conversation_history: List[Dict] = None,
                                        search_query: str = None,
                                        query_rewritten: bool = False) -> str:
        """
        ä½¿ç”¨RAGç”Ÿæˆå›ç­”ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰

        Args:
            query: ç”¨æˆ·åŸå§‹æŸ¥è¯¢
            summaries: ç›¸å…³æ€»ç»“åˆ—è¡¨
            keywords: å…³é”®è¯
            conversation_history: å¯¹è¯å†å²åˆ—è¡¨
            search_query: æ”¹å†™åçš„æ£€ç´¢æŸ¥è¯¢ï¼ˆå¯é€‰ï¼‰
            query_rewritten: æ˜¯å¦ç»è¿‡æŸ¥è¯¢æ”¹å†™

        Returns:
            ç”Ÿæˆçš„å›ç­”
        """
        try:
            system_prompt, user_prompt = self._build_rag_prompts(
                query=query,
                summaries=summaries,
                keywords=keywords,
                conversation_history=conversation_history,
                search_query=search_query,
                query_rewritten=query_rewritten
            )

            logger.info(
                f"è°ƒç”¨AIç”Ÿæˆå›ç­”ï¼ˆRAG+å¯¹è¯å†å²ï¼‰ï¼Œæ€»ç»“æ•°: {len(summaries)}, "
                f"å†å²æ¶ˆæ¯: {len(conversation_history) if conversation_history else 0}, "
                f"æŸ¥è¯¢æ”¹å†™: {query_rewritten}"
            )

            response = client_llm.chat.completions.create(
                model=get_llm_model(),
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7
            )

            answer = response.choices[0].message.content.strip()
            logger.info(f"AIå›ç­”ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(answer)}å­—ç¬¦")

            # æ·»åŠ æ¥æºä¿¡æ¯ï¼ˆè‹¥AIå·²è‡ªè¡Œç”Ÿæˆæ¥æºå—åˆ™ä¸é‡å¤è¿½åŠ ï¼‰
            if "ğŸ“š æ•°æ®æ¥æº" not in answer:
                source_info = self._format_source_info_v3(summaries)
                return f"{answer}\n\n{source_info}"
            return answer

        except Exception as e:
            error_type = type(e).__name__
            error_msg = str(e)

            logger.error(f"AIç”Ÿæˆå›ç­”å¤±è´¥: {error_type}: {error_msg}", exc_info=True)

            if 'Moderation Block' in error_msg or 'content_filter' in error_msg:
                return """âš ï¸ **æŸ¥è¯¢å†…å®¹å—é™**

æŠ±æ­‰ï¼Œä½ çš„æŸ¥è¯¢è§¦å‘äº†å†…å®¹è¿‡æ»¤æœºåˆ¶ï¼Œæ— æ³•æä¾›ç›¸å…³ä¿¡æ¯ã€‚

ğŸ’¡ **å»ºè®®ï¼š**
â€¢ è¯¢é—®é¢‘é“é‡Œæœ€è¿‘çš„æ€»ç»“
â€¢ æŸ¥è¯¢ç‰¹å®šä¸»é¢˜çš„å†å²è®°å½•
â€¢ ä½¿ç”¨ä¸åŒçš„å…³é”®è¯é‡æ–°è¡¨è¿°é—®é¢˜

è¯·é‡æ–°ç»„ç»‡ä½ çš„é—®é¢˜å†è¯•ã€‚"""

            return self._fallback_answer_v3(summaries)

    async def generate_answer_stream(self, query: str,
                                     summaries: List[Dict[str, Any]],
                                     keywords: List[str] = None,
                                     conversation_history: List[Dict] = None,
                                     search_query: str = None,
                                     query_rewritten: bool = False):
        """
        ä½¿ç”¨RAGæµå¼ç”Ÿæˆå›ç­”ï¼ˆå¼‚æ­¥ç”Ÿæˆå™¨ï¼‰

        Args:
            query: ç”¨æˆ·åŸå§‹æŸ¥è¯¢
            summaries: ç›¸å…³æ€»ç»“åˆ—è¡¨
            keywords: å…³é”®è¯åˆ—è¡¨
            conversation_history: å¯¹è¯å†å²
            search_query: æ”¹å†™åçš„æ£€ç´¢æŸ¥è¯¢
            query_rewritten: æ˜¯å¦ç»è¿‡æŸ¥è¯¢æ”¹å†™

        Yields:
            str: é€æ­¥ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        import asyncio

        system_prompt, user_prompt = self._build_rag_prompts(
            query=query,
            summaries=summaries,
            keywords=keywords,
            conversation_history=conversation_history,
            search_query=search_query,
            query_rewritten=query_rewritten
        )

        logger.info(
            f"è°ƒç”¨AIæµå¼ç”Ÿæˆå›ç­”ï¼ˆRAGï¼‰ï¼Œæ€»ç»“æ•°: {len(summaries)}, "
            f"å†å²æ¶ˆæ¯: {len(conversation_history) if conversation_history else 0}, "
            f"æŸ¥è¯¢æ”¹å†™: {query_rewritten}"
        )

        loop = asyncio.get_event_loop()

        def _do_stream():
            return client_llm.chat.completions.create(
                model=get_llm_model(),
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7,
                stream=True
            )

        # åœ¨çº¿ç¨‹æ± ä¸­è°ƒç”¨åŒæ­¥ SDKï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
        stream = await loop.run_in_executor(None, _do_stream)

        full_text = ""
        for chunk in stream:
            if not chunk.choices:
                continue
            delta = chunk.choices[0].delta.content if chunk.choices[0].delta.content else ""
            if delta:
                full_text += delta
                yield delta

        # è¿½åŠ æ¥æºä¿¡æ¯
        if "ğŸ“š æ•°æ®æ¥æº" not in full_text:
            source_info = self._format_source_info_v3(summaries)
            suffix = f"\n\n{source_info}"
            yield suffix

    async def process_query_stream(self, query: str, user_id: int):
        """
        æµå¼å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ˆå¼‚æ­¥ç”Ÿæˆå™¨ç‰ˆæœ¬ï¼‰

        å…ˆå®Œæˆæ£€ç´¢/æ”¹å†™ç­‰é¢„å¤„ç†é˜¶æ®µï¼Œç„¶åæµå¼ yield AI ç”Ÿæˆçš„æ–‡æœ¬ã€‚

        Yields:
            str: æ–‡æœ¬ç‰‡æ®µï¼Œä»¥ "__DONE__" ç»“å°¾è¡¨ç¤ºå®Œæˆï¼Œ
                 ä»¥ "__ERROR__:<msg>" è¡¨ç¤ºå‡ºé”™ï¼Œ
                 ä»¥ "__NEW_SESSION__" è¡¨ç¤ºå¼€å§‹äº†æ–°ä¼šè¯ã€‚
        """
        try:
            logger.info(f"[stream] å¤„ç†æŸ¥è¯¢: user_id={user_id}, query={query}")

            # 1. è·å–æˆ–åˆ›å»ºä¼šè¯
            session_id, is_new_session = self.conversation_mgr.get_or_create_session(user_id)

            # 2. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
            self.conversation_mgr.save_message(
                user_id=user_id,
                session_id=session_id,
                role='user',
                content=query
            )

            # 3. è§£ææŸ¥è¯¢æ„å›¾
            parsed = self.intent_parser.parse_query(query)
            intent = parsed["intent"]

            # 4. éå†…å®¹æŸ¥è¯¢ç›´æ¥è¿”å›ï¼ˆä¸ä½¿ç”¨æµå¼ï¼‰
            if intent == "status":
                answer = await self._handle_status_query()
                yield answer
                self.conversation_mgr.save_message(
                    user_id=user_id, session_id=session_id,
                    role='assistant', content=answer
                )
                yield "__DONE__"
                return

            if intent == "stats":
                answer = await self._handle_stats_query(parsed)
                yield answer
                self.conversation_mgr.save_message(
                    user_id=user_id, session_id=session_id,
                    role='assistant', content=answer
                )
                yield "__DONE__"
                return

            # 5. å†…å®¹æŸ¥è¯¢ï¼šå…ˆå®Œæˆæ£€ç´¢é˜¶æ®µï¼Œå†æµå¼ç”Ÿæˆ
            if is_new_session:
                yield "__NEW_SESSION__"

            original_query = parsed["original_query"]
            keywords = parsed.get("keywords", [])
            time_range = parsed.get("time_range")

            conversation_history = self.conversation_mgr.get_conversation_history(
                user_id, session_id
            )

            # æŸ¥è¯¢æ”¹å†™
            search_query = original_query
            query_rewritten = False
            if (not is_new_session
                    and len(conversation_history) >= 3
                    and PRONOUN_PATTERNS.search(original_query)):
                try:
                    search_query = await self._rewrite_query(original_query, conversation_history)
                    if search_query != original_query:
                        query_rewritten = True
                        logger.info(f"[stream] æŸ¥è¯¢æ”¹å†™: '{original_query}' â†’ '{search_query}'")
                except Exception as e:
                    logger.warning(f"[stream] æŸ¥è¯¢æ”¹å†™å¤±è´¥: {e}")
                    search_query = original_query

            # æ—¶é—´è¿‡æ»¤
            date_after: Optional[str] = None
            if time_range is not None:
                from datetime import datetime, timedelta, timezone
                cutoff = datetime.now(timezone.utc) - timedelta(days=time_range)
                date_after = cutoff.isoformat()

            # è¯­ä¹‰æ£€ç´¢
            semantic_results = []
            if self.vector_store.is_available():
                try:
                    semantic_results = self.vector_store.search_similar(
                        query=search_query, top_k=20, date_after=date_after
                    )
                except Exception as e:
                    logger.error(f"[stream] è¯­ä¹‰æ£€ç´¢å¤±è´¥: {e}")

            # å…³é”®è¯æ£€ç´¢
            keyword_results = []
            if keywords or len(semantic_results) < 5:
                try:
                    search_days = time_range if time_range is not None else 90
                    keyword_results = self.memory_manager.search_summaries(
                        keywords=keywords, time_range_days=search_days, limit=10
                    )
                except Exception as e:
                    logger.error(f"[stream] å…³é”®è¯æ£€ç´¢å¤±è´¥: {e}")

            # èåˆ
            if semantic_results and keyword_results:
                final_candidates = self._rrf_fusion(semantic_results, keyword_results)
            elif semantic_results:
                final_candidates = semantic_results
            elif keyword_results:
                final_candidates = [
                    {
                        'summary_id': r['id'],
                        'summary_text': r['summary_text'],
                        'metadata': {
                            'channel_id': r.get('channel_id'),
                            'channel_name': r.get('channel_name'),
                            'created_at': r.get('created_at')
                        }
                    }
                    for r in keyword_results
                ]
            else:
                if time_range is not None and time_range <= 7:
                    no_result = (
                        f"ğŸ” åœ¨æœ€è¿‘ {time_range} å¤©å†…æœªæ‰¾åˆ°ç›¸å…³æ€»ç»“ã€‚\n\n"
                        f"ğŸ’¡ æç¤ºï¼šå¯ä»¥å°è¯•æ‰©å¤§æ—¶é—´èŒƒå›´ï¼Œä¾‹å¦‚'æœ€è¿‘30å¤©å…³äº...'ã€‚"
                    )
                else:
                    no_result = "ğŸ” æœªæ‰¾åˆ°ç›¸å…³æ€»ç»“ã€‚\n\nğŸ’¡ æç¤ºï¼šå°è¯•è°ƒæ•´å…³é”®è¯æˆ–æ—¶é—´èŒƒå›´ã€‚"
                yield no_result
                self.conversation_mgr.save_message(
                    user_id=user_id, session_id=session_id,
                    role='assistant', content=no_result
                )
                yield "__DONE__"
                return

            # é‡æ’åº
            if self.reranker.is_available() and len(final_candidates) > 5:
                try:
                    final_candidates = self.reranker.rerank(
                        search_query, final_candidates, top_k=5
                    )
                except Exception as e:
                    logger.error(f"[stream] é‡æ’åºå¤±è´¥: {e}")
                    final_candidates = final_candidates[:5]
            else:
                final_candidates = final_candidates[:5]

            # æµå¼ç”Ÿæˆå›ç­”
            full_answer = ""
            try:
                async for chunk in self.generate_answer_stream(
                    query=original_query,
                    summaries=final_candidates,
                    keywords=keywords,
                    conversation_history=conversation_history,
                    search_query=search_query,
                    query_rewritten=query_rewritten
                ):
                    full_answer += chunk
                    yield chunk
            except Exception as e:
                logger.error(f"[stream] AIç”Ÿæˆå¤±è´¥ï¼Œé™çº§åˆ°éæµå¼: {e}")
                fallback = self._fallback_answer_v3(final_candidates)
                yield fallback
                full_answer = fallback

            # ä¿å­˜å®Œæ•´å›ç­”åˆ°å¯¹è¯å†å²
            if is_new_session:
                full_answer = "ğŸƒ *å¼€å§‹æ–°çš„å¯¹è¯ã€‚*\n\n" + full_answer
            self.conversation_mgr.save_message(
                user_id=user_id, session_id=session_id,
                role='assistant', content=full_answer
            )

            yield "__DONE__"

        except Exception as e:
            logger.error(f"[stream] å¤„ç†æŸ¥è¯¢å¤±è´¥: {type(e).__name__}: {e}", exc_info=True)
            yield "__ERROR__:âŒ å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™ï¼Œè¯·ç¨åé‡è¯•ã€‚"

    def _prepare_rag_context(self, summaries: List[Dict[str, Any]]) -> str:
        """
        å‡†å¤‡RAGä¸Šä¸‹æ–‡ä¿¡æ¯

        åŠ¨æ€åˆ†é…æ¯æ¡æ€»ç»“çš„æœ€å¤§å­—ç¬¦æ•°ï¼š
        - 1æ¡ç»“æœ: æœ€å¤š 2000 å­—ç¬¦
        - 2æ¡ç»“æœ: æœ€å¤š 1500 å­—ç¬¦
        - 3-4æ¡ç»“æœ: æœ€å¤š 1000 å­—ç¬¦
        - 5æ¡ç»“æœ: æœ€å¤š 800 å­—ç¬¦
        """
        count = len(summaries[:5])
        if count <= 1:
            max_chars = 2000
        elif count == 2:
            max_chars = 1500
        elif count <= 4:
            max_chars = 1000
        else:
            max_chars = 800

        context_parts = []
        for i, summary in enumerate(summaries[:5], 1):
            metadata = summary.get('metadata', {})
            channel_name = metadata.get('channel_name') or summary.get('channel_name', 'æœªçŸ¥é¢‘é“')
            created_at = metadata.get('created_at') or summary.get('created_at', '')
            summary_text = summary.get('summary_text', '')

            # åŠ¨æ€æˆªæ–­
            text_preview = summary_text[:max_chars] + "..." if len(summary_text) > max_chars else summary_text

            # åˆ†æ•°ä¿¡æ¯
            score_info = ""
            if 'similarity' in summary:
                score_info = f" [ç›¸ä¼¼åº¦: {summary['similarity']:.2f}]"
            if 'rerank_score' in summary:
                score_info += f" [é‡æ’åˆ†: {summary['rerank_score']:.2f}]"

            context_parts.append(
                f"[{i}] {channel_name} ({created_at}){score_info}\n{text_preview}"
            )

        return "\n\n".join(context_parts)

    def _format_source_info_v3(self, summaries: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–æ¥æºä¿¡æ¯ï¼ˆv3ç‰ˆæœ¬ï¼‰"""
        channels = {}
        for s in summaries:
            metadata = s.get('metadata', {})
            channel_id = metadata.get('channel_id') or s.get('channel_id', '')
            channel_name = metadata.get('channel_name') or s.get('channel_name', 'æœªçŸ¥é¢‘é“')

            if channel_id not in channels:
                channels[channel_id] = {
                    'name': channel_name,
                    'count': 0
                }
            channels[channel_id]['count'] += 1

        sources = [f"â€¢ {info['name']}: {info['count']}æ¡"
                  for info in channels.values()]

        return f"ğŸ“š æ•°æ®æ¥æº: {len(sources)}ä¸ªé¢‘é“\n" + "\n".join(sources)

    def _fallback_answer_v3(self, summaries: List[Dict[str, Any]]) -> str:
        """é™çº§æ–¹æ¡ˆï¼šç›´æ¥è¿”å›æ€»ç»“æ‘˜è¦ï¼ˆv3ç‰ˆæœ¬ï¼‰"""
        result = "ğŸ“‹ ç›¸å…³æ€»ç»“æ‘˜è¦ï¼š\n\n"

        for i, summary in enumerate(summaries[:3], 1):
            metadata = summary.get('metadata', {})
            channel_name = metadata.get('channel_name') or summary.get('channel_name', 'æœªçŸ¥é¢‘é“')
            created_at = (metadata.get('created_at') or summary.get('created_at', ''))[:10]
            text = summary.get('summary_text', '')[:300]

            result += f"{i}. **{channel_name}** ({created_at})\n{text}...\n\n"

        return result


# åˆ›å»ºå…¨å±€é—®ç­”å¼•æ“v3å®ä¾‹
qa_engine_v3 = None

def get_qa_engine_v3():
    """è·å–å…¨å±€é—®ç­”å¼•æ“v3å®ä¾‹"""
    global qa_engine_v3
    if qa_engine_v3 is None:
        qa_engine_v3 = QAEngineV3()
    return qa_engine_v3
